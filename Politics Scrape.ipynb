{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95952cac",
   "metadata": {},
   "source": [
    "# ADS 509 Politics Project: APIs and Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8969e",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "185076b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# for the lyrics scrape section\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "import shutil\n",
    "from urllib.parse import urljoin\n",
    "import nbconvert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45c13af3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Politics Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bd7df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_urls = {'CNN':\"https://www.cnn.com/politics\",\n",
    "             'FOX':\"https://www.foxnews.com/politics\"} \n",
    "# we'll use this dictionary to hold both the organization name and the link to their news"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c236c99b",
   "metadata": {},
   "source": [
    "## Part 1: Finding Links to Songs Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d19a5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up a dictionary of lists to hold our links\n",
    "news_pages = defaultdict(list)\n",
    "\n",
    "for org, news_page in news_urls.items() :\n",
    "    # request the page and sleep\n",
    "    r = requests.get(news_page)\n",
    "    time.sleep(5 + 10*random.random())\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        for link in soup.find_all(href=re.compile(\"/politics/\")): \n",
    "            # now extract the links to news pages from this page\n",
    "            news_link = link.get('href')\n",
    "            news_pages[org].append(news_link)\n",
    "\n",
    "# References used in this section:\n",
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "# https://www.geeksforgeeks.org/beautifulsoup-scraping-link-from-html/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c285ec1",
   "metadata": {},
   "source": [
    "Let's make sure we have enough lyrics pages to scrape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae4cda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for org, lp in news_pages.items() :\n",
    "    assert(len(set(lp)) > 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edca10d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For CNN, we have 86 articles.\n",
      "The full pull for this news organization will take roughly 0.24 hours.\n",
      "For FOX, we have 72 articles.\n",
      "The full pull for this news organization will take roughly 0.2 hours.\n"
     ]
    }
   ],
   "source": [
    "# Let's see how long it's going to take to pull these lyrics if we're waiting `5 + 10*random.random()` seconds \n",
    "for org, links in news_pages.items() : \n",
    "    print(f\"For {org}, we have {len(links)} articles.\")\n",
    "    print(f\"The full pull for this news organization will take roughly {round(len(links)*10/3600,2)} hours.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "011be6c6",
   "metadata": {},
   "source": [
    "## Part 2: Pulling Articles\n",
    "\n",
    "Now that we have the links to our article pages, let's go scrape them! Here are the steps for this part. \n",
    "\n",
    "1. Create an empty folder in our repo called \"politics\". \n",
    "1. Iterate over the organizations in `news_pages`. \n",
    "1. Create a subfolder in politics with the site's name. For instance, if the site was CNN you'd have `politics/CNN/` in your repo.\n",
    "1. Iterate over the pages. \n",
    "1. Request the page and extract the articles from the returned HTML file using BeautifulSoup.\n",
    "1. Use the function below, `generate_filename_from_url`, to create a filename based on the article page, then write the article to a text file with that name. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67693711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filename_from_link(link) :\n",
    "    \n",
    "    if not link :\n",
    "        return None\n",
    "    \n",
    "    # drop the http or https and the html\n",
    "    name = link.replace(\"https\",\"\").replace(\"http\",\"\")\n",
    "    name = link.replace(\".html\",\"\")\n",
    "\n",
    "    name = name.replace(\"/politics/\",\"\")\n",
    "    \n",
    "    # Replace useless chareacters with UNDERSCORE\n",
    "    name = name.replace(\"://\",\"\").replace(\".\",\"_\").replace(\"/\",\"_\")\n",
    "    \n",
    "    # tack on .txt\n",
    "    name = name + \".txt\"\n",
    "    \n",
    "    return(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94a78c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the politics folder here, deleting the old folder if one already exists.\n",
    "\n",
    "if os.path.isdir(\"politics\") : \n",
    "    shutil.rmtree(\"politics/\")\n",
    "\n",
    "os.mkdir(\"politics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aee5f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_stub = \"https://www.cnn.com\"\n",
    "fox_stub = \"https://www.foxnews.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d655b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "total_pages = 0 \n",
    "\n",
    "for org, links in news_pages.items() :\n",
    "\n",
    "    # Build a subfolder for the artist\n",
    "    site_folder = os.path.join(\"politics\", org)\n",
    "    os.makedirs(site_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate over the lyrics pages\n",
    "    for link in links:\n",
    "        total_pages += 1\n",
    "\n",
    "        if org == \"CNN\" :\n",
    "            article_url = urljoin(cnn_stub, link)\n",
    "\n",
    "            if \"/2024/\" in article_url :\n",
    "                r_news = requests.get(article_url)\n",
    "                time.sleep(5 + 10 * random.random())\n",
    "\n",
    "                if r_news.status_code == 200:\n",
    "                    soup_news = BeautifulSoup(r_news.text, 'html.parser')\n",
    "                    #title = soup_news.find('div', class_= \"headline_wrapper\").get_text()\n",
    "                    title_element = soup_news.find('h1', class_=\"headline__text inline-placeholder\", id=\"maincontent\")\n",
    "                    if title_element:\n",
    "                        title = title_element.get_text()\n",
    "                    else:\n",
    "                        print(f\"Warning: Title not found for {article_url}\")\n",
    "                        title = \"Untitled\"\n",
    "                    news_paragraphs = soup_news.find_all('p', class_=\"paragraph inline-placeholder\")\n",
    "                    news = '\\n'.join([p.get_text(separator='\\n') for p in news_paragraphs])\n",
    "                    #news = soup_news.find_all('div', class_=None, id=None).get_text(separator='\\n')\n",
    "\n",
    "                    # Write out the title, two returns ('\\n'), and the lyrics. Use `generate_filename_from_url` to generate the filename. \n",
    "                    filename = generate_filename_from_link(link)\n",
    "                    filepath = os.path.join(site_folder, filename)\n",
    "\n",
    "                    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "                        file.write(title + '\\n\\n' + news)\n",
    "        \n",
    "        elif org == \"FOX\" :\n",
    "            article_url = urljoin(fox_stub, link)\n",
    "            if \"/category/\" not in article_url :\n",
    "                r_news = requests.get(article_url)\n",
    "                time.sleep(5 + 10 * random.random())\n",
    "\n",
    "                if r_news.status_code == 200:\n",
    "                    soup_news = BeautifulSoup(r_news.text, 'html.parser')\n",
    "                    #title = soup_news.find('h1', class_= \"headline speakable\").get_text()\n",
    "                    title_element = soup_news.find('h1', class_=\"headline speakable\")\n",
    "                    if title_element:\n",
    "                        title = title_element.get_text()\n",
    "                    else:\n",
    "                        print(f\"Warning: Title not found for {article_url}\")\n",
    "                        title = \"Untitled\"\n",
    "                    #news = return_text_if_not_none(soup_news.find('div', class_= \"article-content\"))\n",
    "                    #news = soup_news.find_all('div', class_= \"article-content\")\n",
    "                    news_paragraphs = soup_news.find_all('p')\n",
    "                    news = '\\n'.join([p.get_text(separator='\\n') for p in news_paragraphs])\n",
    "                    \n",
    "                    # Write out the title, two returns ('\\n'), and the lyrics. Use `generate_filename_from_url` to generate the filename. \n",
    "                    filename = generate_filename_from_link(link)\n",
    "                    filepath = os.path.join(site_folder, filename)\n",
    "                    \n",
    "                    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "                        file.write(title + '\\n\\n' + news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36c394f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run time was 0.32 hours.\n"
     ]
    }
   ],
   "source": [
    "# Find the total run time.\n",
    "print(f\"Total run time was {round((time.time() - start)/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054cf14b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "This assignment asks you to pull data by scraping www.AZLyrics.com.  After you have finished the above sections , run all the cells in this notebook. Print this to PDF and submit it, per the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "217c2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple word extractor from Peter Norvig: https://norvig.com/spell-correct.html\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37778a1c",
   "metadata": {},
   "source": [
    "## Checking Lyrics \n",
    "\n",
    "The output from your lyrics scrape should be stored in files located in this path from the directory:\n",
    "`/lyrics/[Artist Name]/[filename from URL]`. This code summarizes the information at a high level to help the instructor evaluate your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bccac29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For FOX we have 27 files.\n",
      "For FOX we have roughly 23169 words, 3388 are unique.\n",
      "For CNN we have 52 files.\n",
      "For CNN we have roughly 50605 words, 5826 are unique.\n"
     ]
    }
   ],
   "source": [
    "site_folders = os.listdir(\"politics/\")\n",
    "site_folders = [f for f in site_folders if os.path.isdir(\"politics/\" + f)]\n",
    "\n",
    "for org in site_folders : \n",
    "    site_files = os.listdir(\"politics/\" + org)\n",
    "    site_files = [f for f in site_files if 'txt' in f or 'csv' in f or 'tsv' in f]\n",
    "\n",
    "    print(f\"For {org} we have {len(site_files)} files.\")\n",
    "\n",
    "    org_words = []\n",
    "\n",
    "    for f_name in site_files : \n",
    "        with open(\"politics/\" + org + \"/\" + f_name) as infile : \n",
    "            org_words.extend(words(infile.read()))\n",
    "\n",
    "            \n",
    "    print(f\"For {org} we have roughly {len(org_words)} words, {len(set(org_words))} are unique.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
